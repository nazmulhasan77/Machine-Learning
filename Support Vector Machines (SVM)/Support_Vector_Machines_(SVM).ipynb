{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Support Vector Machine (SVM)\n",
        "\n",
        "### üìå What is SVM?\n",
        "\n",
        "**Support Vector Machine (SVM)** is a **supervised learning algorithm** used for **classification** and **regression**.\n",
        "Its core idea is to **find the best boundary (hyperplane)** that separates different classes in the data.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Key Concepts of SVM\n",
        "\n",
        "### 1. **Hyperplane**\n",
        "\n",
        "A **hyperplane** is a line (in 2D), a plane (in 3D), or a higher-dimensional surface that divides the data into classes.\n",
        "\n",
        "* In 2D: a line separates data points.\n",
        "* In 3D: a plane does the separation.\n",
        "\n",
        "The goal of SVM is to find the **optimal hyperplane** that best separates the classes.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Margin**\n",
        "\n",
        "The **margin** is the distance between the hyperplane and the closest data points from each class.\n",
        "\n",
        "* Support Vectors are the data points that lie closest to the hyperplane.\n",
        "* The **optimal hyperplane** is the one with the **maximum margin** (called the **maximum-margin classifier**).\n",
        "\n",
        "This helps improve the model's ability to **generalize**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Support Vectors**\n",
        "\n",
        "These are the **critical data points** that define the decision boundary.\n",
        "If you removed them, the position of the hyperplane would change.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Linearly Separable vs. Non-linearly Separable**\n",
        "\n",
        "* If the data can be separated by a straight line (or plane), it's **linearly separable**.\n",
        "* If not, SVM uses a **kernel trick** to project the data into a **higher-dimensional space** where a linear separator *can* be found.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ SVM Process Overview\n",
        "\n",
        "1. **Plot** the data in feature space.\n",
        "2. **Find the hyperplane** that best separates classes and **maximizes the margin**.\n",
        "3. Use **support vectors** to define the decision boundary.\n",
        "4. If data is not linearly separable, use a **kernel function**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Kernel Trick (for non-linear data)\n",
        "\n",
        "SVM can handle non-linear boundaries using **kernels**, which map data into higher-dimensional spaces.\n",
        "\n",
        "Common kernels:\n",
        "\n",
        "\n",
        "\n",
        "| Kernel         | Function                                 | Use Case                       |\n",
        "|----------------|-------------------------------------------|--------------------------------|\n",
        "| Linear         | K(x, x') = x^T x'                         | Linearly separable data        |\n",
        "| Polynomial     | K(x, x') = (x^T x' + c)^d                 | Polynomial decision boundaries |\n",
        "| RBF (Gaussian) | K(x, x') = exp(-Œ≥ * ||x - x'||^2)         | Complex, non-linear boundaries |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tz-fxiaSCdUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìà **SVM Best Fit Line (Optimal Hyperplane)**\n",
        "\n",
        "In **Support Vector Machine (SVM)**, the goal is to find the **best fit line** (in 2D) or **hyperplane** (in higher dimensions) that:\n",
        "\n",
        "‚úÖ **Separates the classes** (e.g., Class A vs. Class B)\n",
        "‚úÖ **Maximizes the margin** between the closest points of each class\n",
        "‚úÖ Uses only the **support vectors** (closest boundary points) to define this line\n",
        "\n",
        "---\n",
        "\n",
        "### üß† What Is the ‚ÄúBest Fit Line‚Äù in SVM?\n",
        "\n",
        "* It's not just any line that separates the data.\n",
        "* It's the one with the **largest margin** between the two classes.\n",
        "* The margin is the distance between the **hyperplane** and the **nearest data points** (support vectors).\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ Equation of the Best Fit Line (Hyperplane)\n",
        "\n",
        "In 2D, the **hyperplane** is a line:\n",
        "\n",
        "$$\n",
        "w \\cdot x + b = 0\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* `w` is the **weight vector** (direction of the line)\n",
        "* `x` is the **input feature vector**\n",
        "* `b` is the **bias** or intercept\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ SVM Optimization Goal\n",
        "\n",
        "Maximize the **margin**:\n",
        "\n",
        "$$\n",
        "\\text{Margin} = \\frac{2}{\\|w\\|}\n",
        "$$\n",
        "\n",
        "Subject to:\n",
        "\n",
        "$$\n",
        "y_i(w \\cdot x_i + b) \\geq 1 \\quad \\text{for all } i\n",
        "$$\n",
        "\n",
        "This is solved using **convex optimization**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Visual Intuition\n",
        "\n",
        "Imagine you have two groups of dots (red and blue) on a graph.\n",
        "The SVM finds:\n",
        "\n",
        "* A line right **between the groups**\n",
        "* That is as **far away** as possible from the **nearest red and blue dots**\n",
        "* These nearest dots are the **support vectors**\n",
        "---\n"
      ],
      "metadata": {
        "id": "GlX4A0YqFPey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and split data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Train SVM\n",
        "model = SVC(kernel='linear')  # or 'rbf', 'poly'\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eMPWWZkCsRQ",
        "outputId": "f2761f12-8157-441d-a0b2-9e9724cbe3d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Advantages of SVM\n",
        "\n",
        "* Works well in **high-dimensional** spaces.\n",
        "* Effective when **number of features > number of samples**.\n",
        "* Very **robust to overfitting** (especially with a good kernel and regularization).\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå Disadvantages\n",
        "\n",
        "* Can be **computationally intensive** (especially with large datasets).\n",
        "* Not suitable for very **large datasets**.\n",
        "* Requires **feature scaling**.\n",
        "* Choosing the **right kernel** and parameters can be tricky.\n",
        "---\n",
        "\n",
        "## üìå Summary Table\n",
        "\n",
        "| Feature           | Description                         |\n",
        "| ----------------- | ----------------------------------- |\n",
        "| Type              | Supervised Learning                 |\n",
        "| Works for         | Classification and Regression       |\n",
        "| Key Idea          | Maximize margin between classes     |\n",
        "| Handles Nonlinear | Yes (with kernels)                  |\n",
        "| Sensitive to      | Choice of kernel and regularization |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Q7CiV-XSCpTn"
      }
    }
  ]
}